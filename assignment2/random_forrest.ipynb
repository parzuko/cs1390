{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Bagging_Data_Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a text file into a NumPy array.\n",
    "    \n",
    "    :param file_path: Path to the file containing the data.\n",
    "    :return: NumPy array with the loaded data.\n",
    "    \"\"\"\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "def bagging_data(data, n_trees):\n",
    "    \"\"\"\n",
    "    Create bags of data for training each tree in a Random Forest.\n",
    "    \n",
    "    :param data: A NumPy array where rows are samples and the last column is the label.\n",
    "    :param n_trees: The number of trees (bags) in the Random Forest.\n",
    "    :return: A list of NumPy arrays, each array is a bagged sample of the original data.\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    bags = []\n",
    "\n",
    "    for _ in range(n_trees):\n",
    "        # Sampling with replacement\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        bag = data[indices]\n",
    "        bags.append(bag)\n",
    "\n",
    "    return bags\n",
    "\n",
    "# Example usage\n",
    "data = load_data('pa2train.txt')  # Load data from file\n",
    "bags = bagging_data(data, n_trees=100)  # Create 100 bags for the Random Forest\n",
    "\n",
    "# bags now contains 100 different subsets of the original data, each to be used to train a tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for random_forest_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Trees: 200, Best Accuracy: 0.881\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_random_forest(bags, feature_indices, criterion='gini'):\n",
    "    trees = []\n",
    "\n",
    "    for i, bag in enumerate(bags):\n",
    "        features_for_tree = feature_indices[i]\n",
    "        X, y = bag[:, features_for_tree], bag[:, -1]  # Last column is the label\n",
    "\n",
    "        # Reshape data if needed\n",
    "        if X.ndim == 1:  # Check if X is 1D\n",
    "            X = X.reshape(-1, 1)  # Reshape to 2D array\n",
    "\n",
    "        tree = DecisionTreeClassifier(criterion=criterion)\n",
    "        tree.fit(X, y)\n",
    "        trees.append(tree)\n",
    "\n",
    "    return trees\n",
    "\n",
    "\n",
    "def evaluate_random_forest(trees, feature_indices, validation_data):\n",
    "    \"\"\"\n",
    "    Evaluate a Random Forest on validation data.\n",
    "\n",
    "    :param trees: List of trained decision tree classifiers.\n",
    "    :param feature_indices: List of feature indices for each tree.\n",
    "    :param validation_data: Validation dataset.\n",
    "    :return: Accuracy of the Random Forest on the validation dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for tree, features in zip(trees, feature_indices):\n",
    "        X_val = validation_data[:, features]\n",
    "\n",
    "        # Ensure X_val is 2D\n",
    "        if X_val.ndim == 1:\n",
    "            X_val = X_val.reshape(-1, 1)\n",
    "\n",
    "        predictions.append(tree.predict(X_val))\n",
    "    \n",
    "    # Majority voting\n",
    "    predictions = np.array(predictions)\n",
    "    final_prediction = np.round(predictions.mean(axis=0))\n",
    "    accuracy = accuracy_score(validation_data[:, -1], final_prediction)\n",
    "    return accuracy\n",
    "\n",
    "def random_feature_indices(n_features, n_trees, min_features, max_features):\n",
    "    \"\"\"\n",
    "    Generate random feature indices for each tree in the forest.\n",
    "\n",
    "    :param n_features: Total number of features.\n",
    "    :param n_trees: Number of trees in the forest.\n",
    "    :param min_features: Minimum number of features to use for each tree.\n",
    "    :param max_features: Maximum number of features to use for each tree.\n",
    "    :return: List of feature indices for each tree.\n",
    "    \"\"\"\n",
    "    feature_indices = []\n",
    "    all_features = np.arange(n_features)\n",
    "\n",
    "    # Ensuring all features are used at least once\n",
    "    np.random.shuffle(all_features)\n",
    "    feature_indices.extend(all_features[:n_trees])\n",
    "\n",
    "    # Additional random feature selection for remaining trees\n",
    "    for _ in range(n_trees - len(feature_indices)):\n",
    "        n_features_for_tree = np.random.randint(min_features, max_features + 1)\n",
    "        features_for_tree = np.random.choice(all_features, n_features_for_tree, replace=False)\n",
    "        feature_indices.append(features_for_tree)\n",
    "\n",
    "    return feature_indices\n",
    "\n",
    "# Tuning hyperparameters\n",
    "data = load_data('pa2train.txt')\n",
    "validation_data = load_data('pa2validation.txt')\n",
    "n_features = validation_data.shape[1] - 1\n",
    "best_accuracy = 0\n",
    "best_n_trees = 0\n",
    "best_feature_indices = []\n",
    "\n",
    "for n_trees in [10, 50, 100, 150, 200]:\n",
    "    feature_indices = random_feature_indices(n_features, n_trees, 5, 15)\n",
    "    bags = bagging_data(data, n_trees=n_trees)\n",
    "    random_forest = build_random_forest(bags, feature_indices)\n",
    "    accuracy = evaluate_random_forest(random_forest, feature_indices, validation_data)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_n_trees = n_trees\n",
    "        best_feature_indices = feature_indices\n",
    "\n",
    "# Best model\n",
    "print(f\"Best Number of Trees: {best_n_trees}, Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Save best_n_trees to a file\n",
    "with open('best_params.txt', 'w') as file:\n",
    "    file.write(str(best_n_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for decision_tree_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Classifier: DecisionTreeClassifier(max_depth=5)\n",
      "Best Validation Accuracy: 0.884\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a text file into a NumPy array.\n",
    "\n",
    "    :param file_path: Path to the file containing the data.\n",
    "    :return: NumPy array with the loaded data.\n",
    "    \"\"\"\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "def tune_decision_tree(train_data, validation_data):\n",
    "    \"\"\"\n",
    "    Tune a DecisionTreeClassifier using validation data.\n",
    "\n",
    "    :param train_data: Training dataset.\n",
    "    :param validation_data: Validation dataset.\n",
    "    :return: Best classifier and its accuracy.\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_classifier = None\n",
    "\n",
    "    for max_depth in [5, 10, 15, None]:\n",
    "        for min_samples_split in [2, 4, 6, 8]:\n",
    "            for min_samples_leaf in [1, 2, 4, 6]:\n",
    "\n",
    "                # Initialize and train the classifier\n",
    "                clf = DecisionTreeClassifier(criterion='gini', max_depth=max_depth,\n",
    "                                             min_samples_split=min_samples_split,\n",
    "                                             min_samples_leaf=min_samples_leaf)\n",
    "                clf.fit(train_data[:, :-1], train_data[:, -1])\n",
    "\n",
    "                # Evaluate on validation data\n",
    "                predictions = clf.predict(validation_data[:, :-1])\n",
    "                accuracy = accuracy_score(validation_data[:, -1], predictions)\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_classifier = clf\n",
    "\n",
    "    return best_classifier, best_accuracy\n",
    "\n",
    "# Load data\n",
    "train_data = load_data('pa2train.txt')\n",
    "validation_data = load_data('pa2validation.txt')\n",
    "\n",
    "# Tune the classifier\n",
    "best_clf, best_acc = tune_decision_tree(train_data, validation_data)\n",
    "\n",
    "print(f\"Best Decision Tree Classifier: {best_clf}\")\n",
    "print(f\"Best Validation Accuracy: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy on Test Data: 0.876\n",
      "Decision Tree Accuracy on Test Data: 0.885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load test data\n",
    "test_data = load_data('pa2test.txt')\n",
    "\n",
    "# Load and preprocess validation data to get the number of features\n",
    "validation_data = load_data('pa2validation.txt')\n",
    "n_features = validation_data.shape[1] - 1  # Subtract 1 for the label column\n",
    "\n",
    "# Parameters from random forest tuning process\n",
    "best_n_trees_rf = 100  # Replace with the best number of trees found in random forest tuning\n",
    "best_min_features = 5\n",
    "best_max_features = 15\n",
    "\n",
    "# Parameters from decision tree tuning process\n",
    "best_tree, _ = tune_decision_tree(load_data('pa2train.txt'), validation_data)  # Tuning process already selects best tree\n",
    "\n",
    "# Build random forest\n",
    "feature_indices_rf = random_feature_indices(n_features, best_n_trees_rf, best_min_features, best_max_features)\n",
    "bags = bagging_data(load_data('pa2train.txt'), n_trees=best_n_trees_rf)\n",
    "random_forest = build_random_forest(bags, feature_indices_rf)\n",
    "\n",
    "# Evaluate random forest on test data\n",
    "accuracy_rf = evaluate_random_forest(random_forest, feature_indices_rf, test_data)\n",
    "print(f\"Random Forest Accuracy on Test Data: {accuracy_rf}\")\n",
    "\n",
    "# Evaluate decision tree on test data\n",
    "predictions_dt = best_tree.predict(test_data[:, :-1])\n",
    "accuracy_dt = accuracy_score(test_data[:, -1], predictions_dt)\n",
    "print(f\"Decision Tree Accuracy on Test Data: {accuracy_dt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
